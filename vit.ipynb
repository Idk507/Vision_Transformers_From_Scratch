{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "from tqdm import tqdm,trange\n",
    "from torch.nn import CrossEntropyLoss \n",
    "from torch.optim import Adam \n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor \n",
    "from torchvision.datasets.mnist import MNIST \n",
    "from torchvision import transforms\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2d719057230>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIT(nn.Module):\n",
    "    def __init__(self,chw=(1,28,28),n_patches=7,n_heads=2,\n",
    "                 n_blocks=2,hidden_d=2,out_d=10):\n",
    "        super(VIT,self).__init__()\n",
    "\n",
    "        self.chw = chw \n",
    "        self.n_patches = n_patches\n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_heads = n_heads \n",
    "        self.out_d = out_d\n",
    "\n",
    "\n",
    "        #28//7 = 4\n",
    "        assert chw[1]% n_patches == 0, 'Input shape should be divisble by n_patches'\n",
    "        assert chw[2]% n_patches == 0, 'Input shape should be divisble by n_patches'\n",
    "\n",
    "        self.patch_size = (chw[1]/ n_patches, chw[2] / n_patches) #4,4\n",
    "\n",
    "        #linear mapper \n",
    "        self.input_d = int(self.chw[0]*self.patch_size[0]*self.patch_size[1]) #1* 4*4 = 16 \n",
    "        self.linear_mapper = nn.Linear(self.input_d,self.hidden_d)\n",
    "\n",
    "        #learnable classification token\n",
    "        \"\"\"nn. Parameter is used to explicitly specify which tensors should be treated as the model's learnable parameters. \n",
    "        So that those tensors are learned (updated) during the training process to minimize the loss function.\"\"\"\n",
    "        self.class_token = nn.Parameter(torch.randn(1,self.hidden_d))\n",
    "\n",
    "        #positional embedding \n",
    "        self.pos_embed = nn.Parameter(torch.randn(self.n_patches**2 + 1,self.hidden_d))\n",
    "        self.pos_embed.requires_grad = False \n",
    "\n",
    "        #Encoder block \n",
    "        self.encoder_blocks = nn.ModuleList(\n",
    "            [EncoderVIT(self.hidden_d,n_heads) for _ in range(n_blocks)]\n",
    "            )\n",
    "\n",
    "        #classification MLP \n",
    "        self.mlp = nn.Sequential(nn.Linear(self.hidden_d,out_d),\n",
    "                                 nn.Softmax(dim=-1)\n",
    "                                 )\n",
    "\n",
    "    def forward(self,images):\n",
    "\n",
    "        n,c,h,w = images.shape #n,c,h,w ->n,1,28,28\n",
    "        patches = self.patch_embedding(images,self.n_patches) #n,49,16\n",
    "        #print(patches.shape)\n",
    "        tokens = self.linear_mapper(patches.to(images.device)) #n,49,8\n",
    "        tokens = torch.stack([torch.vstack((self.class_token,tokens[i])) for i in range(len(tokens))]) #n,50,8\n",
    "        \n",
    "        pos_embed = self.pos_embed.repeat(n,1,1).to(images.device) #n,50,8\n",
    "        out = tokens + pos_embed\n",
    "\n",
    "        #transformer block \n",
    "        for block in self.encoder_blocks:\n",
    "            out = block(out)\n",
    "            print(out.shape)\n",
    "\n",
    "        #classification token \n",
    "        out = out[:,0] #1,8\n",
    "        \n",
    "        \n",
    "        return self.mlp(out)\n",
    "    \"\"\"The @staticmethod decorator in Python is used to define a method that belongs to a class but does not access any properties or methods of the class.\n",
    "      Here’s a detailed explanation of the role and usage of @staticmethod\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def patch_embedding(images,n_patches): # 7*7 -> n_patches\n",
    "        n,c,h,w = images #n,c,h,w ->n,1,28,28\n",
    "\n",
    "        assert h==w ,'Patch embedding required the dimensions of the height and width to be the same'\n",
    "\n",
    "        patches =torch.zeros(n,n_patches**2,h*w*c//n_patches**2,device = images.device) # h//n_patches,w//n_patches ->28/7 = 4->N,49,16\n",
    "        patch_size = h//n_patches\n",
    "\n",
    "\n",
    "        for idx,image in enumerate(images):\n",
    "            for i in range(n_patches):\n",
    "                for j in range(n_patches):\n",
    "                    patch = image[:,i*patch_size:(i+1)*patch_size,j*patch_size:(j+1)*patch_size]\n",
    "                    #image 2D patch 0--->4 4-----12 \n",
    "                    patches[idx,i*n_patches+j] = patch.flatten()\n",
    "\n",
    "        return patches, #n,49,16 \n",
    "\n",
    "    @staticmethod\n",
    "    def positional_embedding(sequence_length,d): #n,49,8\n",
    "        \"\"\" \n",
    "        p(i,j) ={sin(i/10000^(j/d(emd_dim))) if j is even ,j represent the position of the dimension\n",
    "        cos(i/10000^(j/d(emd_dim))) if j is odd}\n",
    "        \"\"\"\n",
    "        result = torch.ones(sequence_length,d)\n",
    "        for i in range(sequence_length):\n",
    "            for j in range(d): #j->dimension \n",
    "                result[i][j] = np.sin(i /(10000**(j/d))) if j%2 == 0 else np.cos(i/(10000**((j-1) /d)))\n",
    "        \n",
    "        return result \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d,n_heads):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "\n",
    "        self.d = d \n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        assert d% n_heads == 0 ,f'Dimension{d} is not divisble by head : {n_heads}'\n",
    "\n",
    "        #patches --> q,k,v -->n_heads \n",
    "\n",
    "        d_head = int(d/n_heads) #8/2 = 4 \n",
    "        self.q = nn.ModuleList([nn.Linear(d_head,d_head) for _ in range(self.n_heads)]) #(4,4) (4,4)\n",
    "        self.k = nn.ModuleList([nn.Linear(d_head,d_head) for _ in range(self.n_heads)]) #(4,4) (4,4)\n",
    "        self.v = nn.ModuleList([nn.Linear(d_head,d_head) for _ in range(self.n_heads)]) #(4,4) (4,4)\n",
    "        self.d_head = d_head \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward (self,sequences):\n",
    "        #N,sequence_length,token_dim #n,50,8/d heads \n",
    "        #patch 8 / head \n",
    "        result = []\n",
    "        for sequence in sequences:\n",
    "            seq_res = [ ]\n",
    "            for head in range(self.n_heads): #0,1 [2 times]\n",
    "                q_mapping = self.q[head]\n",
    "                k_mapping = self.k[head]\n",
    "                v_mapping = self.v[head]\n",
    "                seq = sequence[: head * self.d_head : (head+1)* self.d_head]\n",
    "                q,k,v = q_mapping(seq),k_mapping(seq),v_mapping(seq)\n",
    "\n",
    "                attention = self.softmax(q@k.T /(self.d_head ** 0.5))\n",
    "                seq_res.append(attention@v)\n",
    "            result.append(torch.hstack(seq_res))\n",
    "        return torch.cat([torch.unsqueeze(r,dim=0) for r in result])\n",
    "\n",
    "\n",
    "\n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderVIT(nn.Module):\n",
    "    def __init__(self,hidden_d,n_heads,mlp_ratio=4):\n",
    "        super(EncoderVIT,self).__init__()\n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_heads = n_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(hidden_d)\n",
    "        self.mhsa = MultiHeadAttention(hidden_d,n_heads)\n",
    "        self.norm2 = nn.LayerNorm(hidden_d)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_d,mlp_ratio*hidden_d),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_ratio * hidden_d,hidden_d)\n",
    "        )\n",
    "\n",
    "        def forward(self,x):\n",
    "            out = x + self.mhsa(self.norm1(x))\n",
    "            out = out + self.mlp(self.norm2(out))\n",
    "            return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 in training: 100%|██████████| 469/469 [07:08<00:00,  1.09it/s]\n",
      "Training:  20%|██        | 1/5 [07:08<28:33, 428.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss : 2.3017357397181133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 in training: 100%|██████████| 469/469 [06:53<00:00,  1.14it/s]\n",
      "Training:  40%|████      | 2/5 [14:01<20:58, 419.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 loss : 2.2900404513263495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 in training: 100%|██████████| 469/469 [20:11<00:00,  2.58s/it]\n",
      "Training:  60%|██████    | 3/5 [34:13<26:02, 781.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 loss : 2.2615771618987455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 in training: 100%|██████████| 469/469 [07:24<00:00,  1.06it/s]\n",
      "Training:  80%|████████  | 4/5 [41:37<10:48, 648.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 loss : 2.2502054364950688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 in training: 100%|██████████| 469/469 [07:21<00:00,  1.06it/s]\n",
      "Training: 100%|██████████| 5/5 [48:59<00:00, 587.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 loss : 2.2429188766967507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing: 100%|██████████| 79/79 [00:44<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss : 2.24631729307054\n",
      "Accuracy : 0.2153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    transform = ToTensor()\n",
    "    train_set = MNIST(root='./datasets', train=True, download=True, transform=transform)\n",
    "    test_set = MNIST(root='./datasets', train=False, download=True, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "    test_loader = DataLoader(test_set, shuffle=True, batch_size=128)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = VIT(chw=(1,28,28), n_patches=7, n_heads=2, n_blocks=2, hidden_d=2, out_d=10).to(device)\n",
    "    \n",
    "    n_epochs = 5 \n",
    "    lr = 0.001\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in trange(n_epochs, desc='Training'):\n",
    "        train_loss = 0.0 \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} in training\"):\n",
    "            x, y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "\n",
    "            train_loss += loss.detach().cpu().item() / len(train_loader)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1} loss : {train_loss}\")\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        test_loss = 0.0\n",
    "        for batch in tqdm(test_loader, desc=\"testing\"):\n",
    "            x, y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "            test_loss += loss.detach().cpu().item() / len(test_loader)\n",
    "            pred = torch.argmax(y_hat, dim=1)\n",
    "            correct += torch.sum(pred == y).detach().cpu().item()\n",
    "            total += len(y)\n",
    "        print(f\"Test loss : {test_loss}\")\n",
    "        print(f\"Accuracy : {correct / total}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idk_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
